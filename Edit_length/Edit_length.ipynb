{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(a,b):\n",
    "    return not a == b\n",
    "\n",
    "def edit_length(x, y, f_sigma):\n",
    "    m, n = len(x), len(y)\n",
    "    edit = [[0]*(n+1) for i in range(m+1)]\n",
    "    \n",
    "    edit[0][0] = (0, \"Nothing\")\n",
    "    for i in range(1,m+1):\n",
    "        edit[i][0] = (i, 1)\n",
    "    for j in range(1,n+1):\n",
    "        edit[0][j] = (j, 2)\n",
    "    \n",
    "    if n == 0 and m == 0:\n",
    "        return edit[0][0][0], edit[0][0][1]\n",
    "    \n",
    "    # 1 delete , 2 insert, 3 change, 4 pass\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            a = edit[i-1][j][0] + 1\n",
    "            b = edit[i][j-1][0] + 1\n",
    "            change_value = f_sigma(x[i-1], y[j-1])\n",
    "            c = edit[i-1][j-1][0] + change_value\n",
    "            if a < b and a < c:\n",
    "                edit[i][j] = (a, 1)\n",
    "            elif b < c:\n",
    "                edit[i][j] = (b, 2)\n",
    "            else:\n",
    "                if change_value:\n",
    "                    edit[i][j] = (c, 3)\n",
    "                else:\n",
    "                    edit[i][j] = (c, 4)\n",
    "\n",
    "    path = []\n",
    "    i = m\n",
    "    j = n\n",
    "    while i != 0 or j != 0:\n",
    "        path_value = edit[i][j][1]\n",
    "        if path_value == 1:\n",
    "            path.append(\"Delete\")\n",
    "            i -= 1\n",
    "        elif path_value == 2:\n",
    "            path.append(\"Insert\")\n",
    "            j -= 1\n",
    "        elif path_value == 3:\n",
    "            path.append(\"Change\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        else:\n",
    "            path.append(\"Pass\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            \n",
    "    path.reverse()\n",
    "    return edit[m][n][0], path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2y_visualisation(x, y):\n",
    "    length, path = edit_length(x, y, sigma)\n",
    "    print(path)\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    print(\"{} => {}\".format(x, y))\n",
    "    for action in path:\n",
    "        if action == \"Insert\":\n",
    "            print(\"{}*{}*{} insert {}\".format(x[0:x_idx],y[y_idx], x[x_idx:], y[y_idx]))\n",
    "            x = x[0:x_idx] + y[y_idx] + x[x_idx:]\n",
    "        elif action == \"Delete\":\n",
    "            print(\"{}*{}*{} delete {}\".format(x[0:x_idx], x[x_idx], x[x_idx+1:], x[x_idx]))\n",
    "            x = x[0:x_idx] + x[x_idx+1:]\n",
    "            x_idx -= 1\n",
    "            y_idx -= 1\n",
    "        elif action == \"Change\":\n",
    "            print(\"{}*{}*{} Change {} -> {}\".format(x[0:x_idx], y[y_idx], x[x_idx+1:], x[x_idx], y[y_idx]))\n",
    "            x = x[0:x_idx] + y[y_idx] + x[x_idx+1:]\n",
    "        x_idx += 1\n",
    "        y_idx += 1\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Insert', 'Pass', 'Pass', 'Change']\n",
      "los => kloc\n",
      "*k*los insert k\n",
      "klo*c* Change s -> c\n",
      "kloc\n"
     ]
    }
   ],
   "source": [
    "x2y_visualisation(\"los\", \"kloc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Change', 'Change', 'Pass', 'Change']\n",
      "Łódź => Lodz\n",
      "*L*ódź Change Ł -> L\n",
      "L*o*dź Change ó -> o\n",
      "Lod*z* Change ź -> z\n",
      "Lodz\n"
     ]
    }
   ],
   "source": [
    "x2y_visualisation(\"Łódź\", \"Lodz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Change', 'Change', 'Pass', 'Pass', 'Pass', 'Pass', 'Insert', 'Pass', 'Pass', 'Pass', 'Pass', 'Delete', 'Change']\n",
      "kwintesencja => quintessence\n",
      "*q*wintesencja Change k -> q\n",
      "q*u*intesencja Change w -> u\n",
      "quinte*s*sencja insert s\n",
      "quintessenc*j*a delete j\n",
      "quintessenc*e* Change a -> e\n",
      "quintessence\n"
     ]
    }
   ],
   "source": [
    "x2y_visualisation(\"kwintesencja\", \"quintessence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pass', 'Pass', 'Pass', 'Pass', 'Change', 'Change', 'Pass', 'Pass', 'Insert', 'Pass', 'Change', 'Change', 'Pass', 'Change', 'Pass', 'Pass', 'Pass', 'Delete', 'Pass']\n",
      "ATGAATCTTACCGCCTCG => ATGAGGCTCTGGCCCCTG\n",
      "ATGA*G*TCTTACCGCCTCG Change A -> G\n",
      "ATGAG*G*CTTACCGCCTCG Change T -> G\n",
      "ATGAGGCT*C*TACCGCCTCG insert C\n",
      "ATGAGGCTCT*G*CCGCCTCG Change A -> G\n",
      "ATGAGGCTCTG*G*CGCCTCG Change C -> G\n",
      "ATGAGGCTCTGGC*C*CCTCG Change G -> C\n",
      "ATGAGGCTCTGGCCCCT*C*G delete C\n",
      "ATGAGGCTCTGGCCCCTG\n"
     ]
    }
   ],
   "source": [
    "x2y_visualisation(\"ATGAATCTTACCGCCTCG\", \"ATGAGGCTCTGGCCCCTG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing subsequence, x_diff, y_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(x, y):\n",
    "    m, n = len(x), len(y)\n",
    "    edit = [[0]*(n+1) for i in range(m+1)]\n",
    "    \n",
    "    edit[0][0] = (0, \"\")\n",
    "    for i in range(1,m+1):\n",
    "        edit[i][0] = (0, 1)\n",
    "    for j in range(1,n+1):\n",
    "        edit[0][j] = (0, 2)\n",
    "    \n",
    "    if n == 0 and m == 0:\n",
    "        return edit[0][0][0], edit[0][0][1]\n",
    "    \n",
    "    # 1 delete , 2 insert, 3 change,4 append lcs (diagonal)\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            a = edit[i-1][j][0]\n",
    "            b = edit[i][j-1][0]\n",
    "            change_value = (x[i-1] == y[j-1])\n",
    "            c = edit[i-1][j-1][0] + change_value\n",
    "            if a > b and a > c:\n",
    "                edit[i][j] = (a, 1)\n",
    "            elif b > c:\n",
    "                edit[i][j] = (b, 2)\n",
    "            else:\n",
    "                if change_value:\n",
    "                    edit[i][j] = (c, 4)\n",
    "                else:\n",
    "                    edit[i][j] = (c, 3)\n",
    "    lcs = []\n",
    "    x_diff = []\n",
    "    y_diff = []\n",
    "    i = m\n",
    "    j = n\n",
    "    while i != 0 or j != 0:\n",
    "        path_value = edit[i][j][1]\n",
    "        if path_value == 1:\n",
    "            x_diff.append(x[i-1])\n",
    "            i -= 1\n",
    "        elif path_value == 2:\n",
    "            y_diff.append(y[j-1])\n",
    "            j -= 1\n",
    "        elif path_value == 3:\n",
    "            x_diff.append(x[i-1])\n",
    "            y_diff.append(y[j-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif path_value == 4:\n",
    "            lcs.append(y[j-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            \n",
    "    lcs.reverse()\n",
    "    x_diff.reverse()\n",
    "    y_diff.reverse()\n",
    "    return edit[m][n][0], \"\".join(lcs), \"\".join(x_diff), \"\".join(y_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'owe', 'rr', 'x')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs(\"rower\", \"oxwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_sigma(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    return int((len(x) + len(y) - edit_length(x,y,lcs_sigma)[0]) / 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs_length(\"rower\", \"oxwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"romeo-i-julia-700.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    lines = file.readlines() # lines\n",
    "    \n",
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "\n",
    "# Tokenize text line by line\n",
    "tokens_1 = [[] for i in range(len(lines))]\n",
    "tokens_2 = [[] for i in range(len(lines))]\n",
    "tokens_number = 0\n",
    "n = len(lines)\n",
    "for i in range(n):\n",
    "    line_tokens = list(nlp.tokenizer(lines[i]))\n",
    "    tokens_1[i] = line_tokens.copy()\n",
    "    tokens_2[i] = line_tokens.copy()\n",
    "    tokens_number += len(line_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens :  3053\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens : \", tokens_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 3% of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_number_after_remove = int(0.97 * tokens_number)\n",
    "while tokens_number != tokens_number_after_remove:\n",
    "    i = random.randrange(len(lines))\n",
    "    j = random.randrange(len(lines))\n",
    "    \n",
    "    n = len(tokens_1[i])\n",
    "    m = len(tokens_2[j])\n",
    "    \n",
    "    if n == 0 or m == 0:\n",
    "        continue\n",
    "    \n",
    "    k = random.randrange(n)\n",
    "    l = random.randrange(m)\n",
    "    \n",
    "    if tokens_1[i][k].text == '\\n' or tokens_2[j][l].text == '\\n':\n",
    "        continue\n",
    "    \n",
    "    tokens_1[i].pop(k)\n",
    "    tokens_2[j].pop(l)\n",
    "    \n",
    "    tokens_number -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest common subsequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2877"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1 = []\n",
    "t_2 = []\n",
    "for i in range(len(lines)):\n",
    "    t_1 += tokens_1[i]\n",
    "    t_2 += tokens_2[i]\n",
    "\n",
    "lcs_length(t_1, t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff_line(diff, index, file):\n",
    "    if len(diff) > 0:\n",
    "        if diff[len(diff) - 1] == '\\n': # remove end of line in diff if n < m\n",
    "            diff = diff[:-1]\n",
    "        if len(diff) > 0:\n",
    "            print(\"<line:{}, '{}'>:{}\".format(index, file, diff))\n",
    "\n",
    "def diff(file_1, file_2):\n",
    "    with open(file_1, \"r\", encoding=\"utf8\") as f:\n",
    "        lines_1 = f.readlines()\n",
    "        \n",
    "    with open(file_2, \"r\", encoding=\"utf8\") as f:\n",
    "        lines_2 = f.readlines()\n",
    "    \n",
    "    n = len(lines_1)\n",
    "    m = len(lines_2)\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        if i < m:\n",
    "            lcs_length, lcs_subsequence, x_diff, y_diff = lcs(lines_1[i], lines_2[i])\n",
    "            print_diff_line(x_diff, i, file_1)\n",
    "            print_diff_line(y_diff, i, file_2)\n",
    "        else:\n",
    "            print_diff_line(lines_1[i], i, file_1) # print rest of the first file\n",
    "        \n",
    "    for i in range(n, m):\n",
    "        print_diff_line(lines_2[i], i, file_2) # print rest of the second file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_1.txt:\n",
    "<br>rower\n",
    "<br>abcdefgh\n",
    "<br>jojojo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_2.txt:\n",
    "<br>oxwe\n",
    "<br>bcxxdefgh\n",
    "<br>ojyyjo\n",
    "<br>ergAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<line:0, 'text_1.txt'>:rr\n",
      "<line:0, 'text_2.txt'>:x\n",
      "<line:1, 'text_1.txt'>:a\n",
      "<line:1, 'text_2.txt'>:xx\n",
      "<line:2, 'text_1.txt'>:jo\n",
      "<line:2, 'text_2.txt'>:yy\n",
      "<line:3, 'text_2.txt'>:ergAGE\n"
     ]
    }
   ],
   "source": [
    "diff(\"text_1.txt\", \"text_2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zad9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tokenized text to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens_1.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "    for line_tokens in tokens_1:\n",
    "        f.write(\" \".join(map(str,line_tokens)))\n",
    "    \n",
    "with open(\"tokens_2.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "    for line_tokens in tokens_2:\n",
    "        f.write(\" \".join(map(str,line_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<line:2, 'tokens_2.txt'>:Romeo \n",
      "<line:5, 'tokens_1.txt'>:88 -\n",
      "<line:5, 'tokens_2.txt'>: -903\n",
      "<line:11, 'tokens_2.txt'>: szlachetnego\n",
      "<line:13, 'tokens_1.txt'>:  \n",
      "<line:13, 'tokens_2.txt'>: —\n",
      "<line:17, 'tokens_2.txt'>: —\n",
      "<line:18, 'tokens_1.txt'>: *\n",
      "<line:20, 'tokens_1.txt'>:  \n",
      "<line:21, 'tokens_1.txt'>:  \n",
      "<line:25, 'tokens_1.txt'>:  \n",
      "<line:28, 'tokens_1.txt'>: Montekiego\n",
      "<line:29, 'tokens_2.txt'>: KAPULET\n",
      "<line:51, 'tokens_1.txt'>: z\n",
      "<line:53, 'tokens_2.txt'>: rodzicielskie\n",
      "<line:57, 'tokens_2.txt'>: ,\n",
      "<line:58, 'tokens_1.txt'>: przedstawienia\n",
      "<line:61, 'tokens_2.txt'>: …\n",
      "<line:82, 'tokens_1.txt'>: bobyśmy\n",
      "<line:97, 'tokens_2.txt'>: zwyczaj\n",
      "<line:117, 'tokens_1.txt'>:a kżdego\n",
      "<line:127, 'tokens_2.txt'>: od\n",
      "<line:132, 'tokens_2.txt'>:t ylko\n",
      "<line:142, 'tokens_1.txt'>:z predsiębrać\n",
      "<line:157, 'tokens_1.txt'>:SAMSON \n",
      "<line:159, 'tokens_1.txt'>: dobyty .\n",
      "<line:162, 'tokens_1.txt'>:GRZEGORZ \n",
      "<line:179, 'tokens_1.txt'>: za\n",
      "<line:184, 'tokens_2.txt'>: ;\n",
      "<line:187, 'tokens_2.txt'>:SAMSON \n",
      "<line:194, 'tokens_2.txt'>:i pane\n",
      "<line:204, 'tokens_2.txt'>:Czy \n",
      "<line:211, 'tokens_2.txt'>: jest\n",
      "<line:216, 'tokens_2.txt'>: .\n",
      "<line:221, 'tokens_1.txt'>:ie skrzywłm\n",
      "<line:226, 'tokens_2.txt'>: Abrahama\n",
      "<line:233, 'tokens_1.txt'>: nie .\n",
      "<line:248, 'tokens_1.txt'>: będzie\n",
      "<line:260, 'tokens_1.txt'>:SAMSON \n",
      "<line:265, 'tokens_2.txt'>:ABRAHAM \n",
      "<line:270, 'tokens_1.txt'>:SAMSON \n",
      "<line:272, 'tokens_2.txt'>: oswim\n",
      "<line:279, 'tokens_2.txt'>: /\n",
      "<line:281, 'tokens_1.txt'>: /\n",
      "<line:286, 'tokens_2.txt'>:Cóż \n",
      "<line:293, 'tokens_2.txt'>: nim\n",
      "<line:301, 'tokens_1.txt'>: nikczemny\n",
      "<line:303, 'tokens_2.txt'>: .\n",
      "<line:309, 'tokens_1.txt'>:z \n",
      "<line:311, 'tokens_2.txt'>: iPan\n",
      "<line:325, 'tokens_2.txt'>:KAPULET \n",
      "<line:327, 'tokens_1.txt'>: !\n",
      "<line:327, 'tokens_2.txt'>: Monteki\n",
      "<line:328, 'tokens_1.txt'>: .\n",
      "<line:328, 'tokens_2.txt'>: szydnie\n",
      "<line:335, 'tokens_1.txt'>: !\n",
      "<line:352, 'tokens_1.txt'>: stali\n",
      "<line:352, 'tokens_2.txt'>:Bezcześciciele \n",
      "<line:353, 'tokens_1.txt'>:Czy \n",
      "<line:358, 'tokens_2.txt'>:e tgo\n",
      "<line:363, 'tokens_2.txt'>: poważni\n",
      "<line:366, 'tokens_2.txt'>:e dłoni\n",
      "<line:367, 'tokens_2.txt'>:y zardzewiałm\n",
      "<line:369, 'tokens_1.txt'>:ś wań\n",
      "<line:369, 'tokens_2.txt'>: ,\n",
      "<line:370, 'tokens_2.txt'>:cie żym\n",
      "<line:373, 'tokens_1.txt'>: zaś\n",
      "<line:375, 'tokens_1.txt'>:a m\n",
      "<line:377, 'tokens_1.txt'>: śmierci\n",
      "<line:379, 'tokens_1.txt'>: obywatele\n",
      "<line:385, 'tokens_1.txt'>: ?\n",
      "<line:390, 'tokens_2.txt'>:naszego \n",
      "<line:394, 'tokens_1.txt'>: zionąc\n",
      "<line:395, 'tokens_2.txt'>: nim\n",
      "<line:396, 'tokens_2.txt'>:Które \n",
      "<line:397, 'tokens_1.txt'>: zamachów\n",
      "<line:400, 'tokens_2.txt'>: i\n",
      "<line:403, 'tokens_2.txt'>:NI MOTEK\n",
      "<line:405, 'tokens_1.txt'>: Romeo\n",
      "<line:406, 'tokens_2.txt'>: ,\n",
      "<line:412, 'tokens_1.txt'>:się\n",
      "<line:412, 'tokens_2.txt'>:oknach\n",
      "<line:414, 'tokens_2.txt'>: ów\n",
      "<line:417, 'tokens_2.txt'>:Ledwiem \n",
      "<line:419, 'tokens_1.txt'>: w\n",
      "<line:421, 'tokens_1.txt'>: (\n",
      "<line:422, 'tokens_2.txt'>: )\n",
      "<line:424, 'tokens_2.txt'>: w\n",
      "<line:425, 'tokens_1.txt'>:me\n",
      "<line:425, 'tokens_2.txt'>:ukał\n",
      "<line:436, 'tokens_1.txt'>: ,\n",
      "<line:437, 'tokens_1.txt'>: pokoju\n",
      "<line:438, 'tokens_2.txt'>: dnia\n",
      "<line:446, 'tokens_2.txt'>: powód\n",
      "<line:451, 'tokens_2.txt'>: niego\n",
      "<line:463, 'tokens_1.txt'>: w\n",
      "<line:466, 'tokens_1.txt'>: swój kielich\n",
      "<line:466, 'tokens_2.txt'>:Nim \n",
      "<line:467, 'tokens_1.txt'>: przed\n",
      "<line:469, 'tokens_1.txt'>:o śrdka\n",
      "<line:471, 'tokens_1.txt'>: głębi\n",
      "<line:476, 'tokens_1.txt'>:o nadchdzi\n",
      "<line:480, 'tokens_2.txt'>:MONTEKI \n",
      "<line:488, 'tokens_2.txt'>:BENWOLIO \n",
      "<line:490, 'tokens_2.txt'>: .\n",
      "<line:495, 'tokens_1.txt'>:                          nie\n",
      "<line:500, 'tokens_2.txt'>:a bił\n",
      "<line:503, 'tokens_1.txt'>:ROMEO \n",
      "<line:507, 'tokens_1.txt'>: zboczyli\n",
      "<line:510, 'tokens_2.txt'>:BENWOLIO \n",
      "<line:512, 'tokens_1.txt'>: dłuży\n",
      "<line:525, 'tokens_1.txt'>:ROMEO \n",
      "<line:532, 'tokens_2.txt'>: to\n",
      "<line:535, 'tokens_1.txt'>:ROMEO \n",
      "<line:537, 'tokens_1.txt'>: jej\n",
      "<line:537, 'tokens_2.txt'>: skąd\n",
      "<line:549, 'tokens_1.txt'>:e cl\n",
      "<line:550, 'tokens_1.txt'>:jeć\n",
      "<line:550, 'tokens_2.txt'>:dzi\n",
      "<line:552, 'tokens_1.txt'>: imłość\n",
      "<line:553, 'tokens_2.txt'>: !\n",
      "<line:557, 'tokens_1.txt'>: chaosie\n",
      "<line:558, 'tokens_2.txt'>: !\n",
      "<line:560, 'tokens_2.txt'>:Taką \n",
      "<line:561, 'tokens_1.txt'>: się\n",
      "<line:566, 'tokens_2.txt'>: ,\n",
      "<line:576, 'tokens_1.txt'>: ,\n",
      "<line:577, 'tokens_2.txt'>: duszy\n",
      "<line:583, 'tokens_2.txt'>: przez\n",
      "<line:585, 'tokens_2.txt'>:ię powkszasz\n",
      "<line:588, 'tokens_2.txt'>: Miłość\n",
      "<line:591, 'tokens_1.txt'>: którym tonie\n",
      "<line:593, 'tokens_2.txt'>:Żółcią \n",
      "<line:596, 'tokens_1.txt'>:e odjść\n",
      "<line:596, 'tokens_2.txt'>:/ \n",
      "<line:608, 'tokens_1.txt'>: co\n",
      "<line:613, 'tokens_1.txt'>: to\n",
      "<line:616, 'tokens_2.txt'>:ROMEO \n",
      "<line:629, 'tokens_2.txt'>:ROMEO \n",
      "<line:637, 'tokens_2.txt'>:BENWOLIO \n",
      "<line:639, 'tokens_1.txt'>:                         m ierzył\n",
      "<line:654, 'tokens_2.txt'>:ROMEO \n",
      "<line:657, 'tokens_1.txt'>: ;\n",
      "<line:658, 'tokens_1.txt'>:d twarą\n",
      "<line:658, 'tokens_2.txt'>: wstydliwości swojej\n",
      "<line:659, 'tokens_2.txt'>: się\n",
      "<line:660, 'tokens_1.txt'>: ;\n",
      "<line:660, 'tokens_2.txt'>:Szydzi \n",
      "<line:662, 'tokens_1.txt'>:e zjdna .\n",
      "<line:663, 'tokens_2.txt'>:ie bdna\n",
      "<line:668, 'tokens_1.txt'>:BENWOLIO \n",
      "<line:677, 'tokens_2.txt'>:Całą \n",
      "<line:679, 'tokens_1.txt'>: mądrze\n",
      "<line:680, 'tokens_2.txt'>: nie\n",
      "<line:686, 'tokens_1.txt'>: na\n",
      "<line:691, 'tokens_1.txt'>:Doradź \n",
      "<line:697, 'tokens_2.txt'>:c ozom\n",
      "<line:698, 'tokens_2.txt'>:Rozpatrywania \n"
     ]
    }
   ],
   "source": [
    "diff(\"tokens_1.txt\", \"tokens_2.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
